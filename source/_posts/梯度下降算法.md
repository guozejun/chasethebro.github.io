---
title: 梯度下降算法
toc: true
date: 2021-03-04 15:02:22
tags: [神经网络]
categories:
    - 图神经网络
---

## 基本概念

梯度下降法（Gradient descent）是一个一阶最优化算法，通常也称为最陡下降法，但是不该与近似积分的最陡下降法（Method of steepest descent）混淆。 要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。如果相反地向梯度正方向迭代进行搜索，则会接近函数的局部极大值点；这个过程则被称为梯度上升法。

<!-- more -->

### 梯度

梯度是多元导数的概括。平常的一元（单变量）函数的导数是标量值函数，而多元函数的梯度是向量值函数。多元可微函数$f$在点$P$上的梯度，是以$f$在$P$上的偏导数为分量的向量。

### 散度

散度（divergence）或称发散度，是向量分析中的一个向量算子，将向量空间上的一个向量场（矢量场）对应到一个标量场上。散度描述的是向量场里一个点是汇聚点还是发源点，形象地说，就是这包含这一点的一个微小体元中的向量是“向外”居多还是“向内”居多。

### 拉普拉斯算子

在数学以及物理中，拉普拉斯算子或是拉普拉斯算符（Laplace operator, Laplacian）是由欧几里得空间中的一个函数的梯度的散度给出的微分算子.

## 参考资料
> - 《深入浅出图神经网络》
